import streamlit as st
import asyncio
import os
from core.spider.bilibili.bilibili_comment import Video_Comment_Extractor
from core.spider.bilibili.CommentWriter import CommentWriter
from core.spider.bilibili.CommentAnalyser import CommentAnalyser


def render_bilibili_page():
    st.title("ğŸ“º Bilibili è¯„è®ºå…¨è‡ªåŠ¨é‡‡é›†ä¸åˆ†æ")

    #------ ä¾§è¾¹æ  ------
    # Cookie ç®¡ç†
    with st.sidebar:
        st.header("ğŸ”‘ è´¦æˆ·é…ç½®")

        # å®ä¾‹åŒ–ä¸´æ—¶Extractor ç”¨æ¥æ£€æŸ¥Cookie
        # ä¼ å…¥TEST_LINKä»…ä¸ºåˆå§‹åŒ–,ä¸ä»£è¡¨æœ€ç»ˆçˆ¬å–ç›®æ ‡
        temp_extractor = Video_Comment_Extractor("https://www.bilibili.com/video/BV1qt411j7fV")
        temp_extractor.get_cookies()


        if temp_extractor.check_login_status():
            st.success(f"å·²ç™»é™†:{temp_extractor.uname}")
            if st.button("æ›´æ–°Cookie"):
                st.session_state.show_cookie_input = True
            
        else:
            st.warning("å½“å‰ä¸ºåŒ¿åæ¨¡å¼(æ— æ³•æŠ“å–IPå±åœ°)")
            st.session_state.show_cookie_input = True

        if st.session_state.get('show_cookie_input'):
            new_cookie = st.text_area("ç²˜è´´æ–°çš„ Cookie å­—ç¬¦ä¸²:", help="ä»æµè§ˆå™¨æ§åˆ¶å°è·å–çš„ raw string")
            if st.button("ä¿å­˜å¹¶éªŒè¯"):
                if new_cookie:
                    temp_extractor.save_cookie_to_file(new_cookie)
                    st.rerun() # é‡æ–°åŠ è½½é¡µé¢ä»¥è§¦å‘ get_cookies
                else:
                    st.error("è¯·è¾“å…¥å†…å®¹")

        # ------ä¸»ç•Œé¢------ä»»åŠ¡è¾“å…¥
    st.subheader("ğŸš€ é‡‡é›†ä»»åŠ¡")
    url_input = st.text_area("è¾“å…¥è§†é¢‘é“¾æ¥(å¤šä¸ªé“¾æ¥è¯·æ¢è¡Œ)",placeholder="https://www.bilibili.com/video/BV1xxxx\nhttps://www.bilibili.com/video/BV2xxxx")

    col1,col2 = st.columns([1,4])
    with col1:
        max_concurrency = st.number_input("å¹¶å‘æ•°",1,10,5)

    with col2:
        order_type = st.selectbox("æ’åºæ–¹å¼",["time","hot"],help = "å»ºè®®é€‰æ‹© time çˆ¬å–æ›´å®‰å…¨")

    if st.button("å¼€å§‹æ‰¹é‡æ‰§è¡Œ"):
        urls = [u.strip() for u in url_input.split('\n') if u.strip()]
        if not urls:
            st.error("è¯·è¾“å…¥æœ‰æ•ˆçš„é“¾æ¥!")
            return 
                
        for idx,url in enumerate(urls):
            st.divider()
            st.markdown(f"#### æ­£åœ¨å¤„ç† {idx+1} / {len(urls)} ä¸ªè§†é¢‘")
            st.caption("URL:{url}")


                    # å‡†å¤‡çˆ¬å–
            try:
                extractor = Video_Comment_Extractor(url)

                from core.spider.utils import default_filename
                        # è‡ªå®šä¹‰æ–‡ä»¶å(BVå·+æ—¶é—´æˆ³)
                fname  = f"bili_{extractor.extract_bv_id() or 'video'}_{idx}"
                writer = CommentWriter(fname)

                        # åˆ›å»ºUIè¿›åº¦ç»„ä»¶
                p_bar = st.progress(0,text = "å‡†å¤‡çˆ¬å–...")
                status_text = st.empty()

                        # å®šä¹‰è¿›åº¦å›è°ƒå‡½æ•°
                def update_ui(ratio,msg):
                    p_bar.progress(min(ratio,1.0),text = msg)



                        # å¼‚æ­¥çˆ¬å–
                asyncio.run(extractor.crawl_all_comments_async(
                                writer = writer,
                                order = order_type,
                                callback=update_ui
                        ))
                            
                p_bar.progress(1.0,text = "âœ… é‡‡é›†å®Œæˆï¼æ­£åœ¨å¯åŠ¨åˆ†æ...")
                writer.close()


                        # é“¾å¼åˆ†æ
                st.info("ğŸ“Š æ­£åœ¨ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
                analyzer = CommentAnalyser(writer.filepath)
                analyzer.load()
                analyzer.preprocess


                        # å±•ç¤ºç»Ÿè®¡ç»“æœ
                stats = analyzer.analyze_basic()
                c1,c2,c3 = st.columns(3)
                c1.metric("æ€»è¯„è®ºæ•°", stats['total_comments'])
                c2.metric("å¹³å‡ç‚¹èµ", f"{stats['avg_like']:.1f}")
                c3.metric("å¹³å‡å›å¤", f"{stats['avg_reply']:.1f}")


                        # å±•ç¤ºå›¾è¡¨
                t1,t2,t3 = st.tabs(["è¯äº‘å›¾","è¯„è®ºæ—¶é—´å¯†åº¦","ç”¨æˆ·åˆ†å¸ƒ"])
                with t1:
                    fig_wc = analyzer.plot_wordcloud(return_fig = True)
                    st.pyplot(fig_wc)

                with t2:
                    fig_time = analyzer.plot_time_density(return_fig = True)
                    st.pyplot(fig_time)

                            # å±•ç¤ºéƒ¨åˆ†å…³é”®è¯
                    st.write("Top å…³é”®è¯:",analyzer.get_keywords(5))
                with t3:
                    fig_ip = analyzer.plot_ip_distribution(deduplicate = True,return_fig = True)
                    st.pyplot(fig_ip)

                    fig_lv = analyzer.plot_user_level(return_fig = True)
                    st.pyplot(fig_lv)

            except Exception as e:
                st.error(f"å¤„ç†è§†é¢‘{url}æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")


if __name__ == "__main__":
    # è°ƒç”¨æ¸²æŸ“
    render_bilibili_page()


